{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as to\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import scipy.spatial as spatial \n",
    "from numpy.matlib import repmat \n",
    "import matplotlib.pyplot as plt \n",
    "from pyface.api import GUI \n",
    "import math \n",
    "import numpy.random as ra \n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torchvision\n",
    "import cv2 as cv\n",
    "\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from scipy.ndimage import rotate\n",
    "from torchvision.transforms.functional import rotate as trot\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "device = to.device('cuda') if to.cuda.is_available() else to.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(path):\n",
    "    \"\"\"\n",
    "    Helper function to get annotations from the json files\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "            \n",
    "        anns = []\n",
    "        bbox = []\n",
    "        files = []\n",
    "        for ann in annotations['data']:\n",
    "            temp = []\n",
    "            landmarks = ann['landmarks']\n",
    "            files.append(ann['file'])\n",
    "            for i in range(1,len(landmarks),2):\n",
    "                temp.append([landmarks[i-1], landmarks[i]])\n",
    "                \n",
    "            anns.append(temp)\n",
    "            bbox.append(ann['bbox'])\n",
    "                \n",
    "        anns = np.array(anns)\n",
    "        bbox = np.array(bbox)\n",
    "        files = np.array(files)\n",
    "    return anns,bbox,files\n",
    "\n",
    "def vector_to_heatmaps(keypoints):\n",
    "    \"\"\"\n",
    "    Creates heatmaps from keypoint locations for a single image.\n",
    "    Input: array of size N_KEYPOINTS x 2\n",
    "    Output: array of size N_KEYPOINTS x MODEL_IMG_SIZE x MODEL_IMG_SIZE\n",
    "    \"\"\"\n",
    "    heatmaps = np.zeros([N_KEYPOINTS, MODEL_IMG_SIZE, MODEL_IMG_SIZE])\n",
    "    for k, (x,y) in enumerate(keypoints):\n",
    "        x, y = int(x), int(y)\n",
    "        if (0 < x < MODEL_IMG_SIZE) and (0 < y < MODEL_IMG_SIZE):\n",
    "            heatmaps[k, int(y), int(x)] = 1\n",
    "            \n",
    "    heatmaps = blur_heatmaps(heatmaps)\n",
    "    return heatmaps\n",
    "    \n",
    "    \n",
    "def blur_heatmaps(heatmaps):\n",
    "    \"\"\"\n",
    "    Blurs the heatmaps using GaussianBlur of defined size\n",
    "    \"\"\"\n",
    "    heatmaps_blurred = heatmaps.copy()\n",
    "    for k in range(len(heatmaps)):\n",
    "        if heatmaps_blurred[k].max() == 1:\n",
    "            heatmaps_blurred[k] = cv.GaussianBlur(heatmaps[k], (51,51), 3)\n",
    "            heatmaps_blurred[k] = heatmaps_blurred[k] / heatmaps_blurred[k].max()\n",
    "    \n",
    "    return heatmaps_blurred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Architecture and Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape=False\n",
    "\n",
    "#bin gradients here\n",
    "grad_bins = 8\n",
    "\n",
    "#configure the network: filter size\n",
    "ksz = (5,5)\n",
    "kha = (np.floor(ksz[0]/2).astype(int),np.floor(ksz[1]/2).astype(int))\n",
    "    \n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_depth, out_depth):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_depth),\n",
    "            nn.Conv2d(in_depth, out_depth, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_depth),\n",
    "            nn.Conv2d(out_depth, out_depth, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "#so here it is:\n",
    "def OGCNN(im,bins,k1):\n",
    "    #inputs: im: gray or single channel\n",
    "    #bins: 0:8 segmented images\n",
    "    #k1: filter\n",
    "    \n",
    "    #out: imsz - (ksz-1)\n",
    "    \n",
    "    \n",
    "    ksz = k1.shape[1:] #extra 1 dim just as technicality \n",
    "    kha = (np.floor(ksz[0]/2).astype(int), np.floor(ksz[1]/2).astype(int))\n",
    "    \n",
    "    ks = to.zeros(8,ksz[0],ksz[1])\n",
    "    ks[0] = k1\n",
    "    \n",
    "    #8 rotations:\n",
    "    for i in range(1,8):\n",
    "        ks[i] = trot(k1, 45*i, torchvision.transforms.functional.InterpolationMode.BILINEAR)\n",
    "        \n",
    "    # bascially, zero-padding\n",
    "    imsz = im.shape\n",
    "    im2 = to.zeros(imsz[0]+2*kha[0],imsz[1]+2*kha[1])\n",
    "    im2[kha[0]:kha[0]+imsz[0],kha[1]:kha[1]+imsz[1]] = im\n",
    "    \n",
    "    #flatten convolutional neighbors\n",
    "    flat = to.zeros(imsz[0]*imsz[1],ksz[0]*ksz[1])\n",
    "    for i in range(ksz[0]):\n",
    "        for j in range(ksz[1]):\n",
    "            flat[:,ksz[1]*i + j] = im2[i:i+imsz[0],j:j+imsz[1]].flatten()\n",
    "                \n",
    "    labels = bins.flatten()\n",
    "    flatout = to.zeros(imsz[0]*imsz[1])\n",
    "        \n",
    "    #apply convolutional filters\n",
    "    for i in range(8):\n",
    "        l = labels==i\n",
    "        flatout[l] = flat[l,:]@(ks[i].flatten())\n",
    "        \n",
    "    out = flatout.unflatten(0,imsz)[kha[0]:-kha[0],kha[1]:-kha[1]] #prune off ends \n",
    "    #out = flatout.unflatten(0,imsz)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def OGCNN2(im,bins,k1):\n",
    "    '''multiple filts at once'''\n",
    "    \n",
    "    #inputs: im: gray or single channel\n",
    "    #bins: 0:8 segmented images\n",
    "    '''k1: n*row*col filters (n filt total)'''\n",
    "    \n",
    "    #out: imsz - (ksz-1)\n",
    "    \n",
    "    \n",
    "    ksz = k1.shape[1:] #first dim stores # filt\n",
    "    nks = k1.shape[0]\n",
    "    \n",
    "    kha = (np.floor(ksz[0]/2).astype(int), np.floor(ksz[1]/2).astype(int))\n",
    "    \n",
    "    ks = to.zeros(8,nks,ksz[0],ksz[1]).to(device)\n",
    "    \n",
    "    ks[0] = k1\n",
    "    \n",
    "    #8 rotations:\n",
    "    for i in range(1,8):\n",
    "        ks[i] = trot(k1, 45*i, torchvision.transforms.functional.InterpolationMode.BILINEAR)\n",
    "        \n",
    "    imsz = im.shape\n",
    "    im2 = to.zeros(imsz[0]+2*kha[0],imsz[1]+2*kha[1]).to(device)\n",
    "    im2[kha[0]:kha[0]+imsz[0],kha[1]:kha[1]+imsz[1]] = im\n",
    "    \n",
    "    flat = to.zeros(imsz[0]*imsz[1],ksz[0]*ksz[1]).to(device) #flatten convolutional neighbors\n",
    "    for i in range(ksz[0]):\n",
    "        for j in range(ksz[1]):\n",
    "            flat[:,ksz[1]*i + j] = im2[i:i+imsz[0],j:j+imsz[1]].flatten()\n",
    "                \n",
    "    labels = bins.flatten()\n",
    "    #\n",
    "    flatout = to.zeros(imsz[0]*imsz[1],nks).to(device) #now life gets complicated\n",
    "        \n",
    "    for i in range(8):\n",
    "        l = labels==i\n",
    "        flatout[l,:] = flat[l,:]@(ks[i].flatten(1,2).T) #apply convolutional filters\n",
    "        \n",
    "\n",
    "    out = flatout.unflatten(0,imsz).permute(2,0,1) #[kha[0]:-kha[0],kha[1]:-kha[1]] \n",
    "    '''prune off ends?????????'''\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "def focal_loss(pred,tru):\n",
    "    \"\"\"\n",
    "    Focal loss calculated over 17 output channels\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "    gamma = 3\n",
    "    input_soft = F.softmax(pred, dim=0) + eps\n",
    "    alpha = 1\n",
    "\n",
    "    # compute the actual focal loss\n",
    "    weight = to.pow(1. - input_soft, gamma)\n",
    "    focal = -alpha * weight * to.log(input_soft)\n",
    "    loss_tmp = to.sum(tru*focal, dim=1)\n",
    "\n",
    "    loss = to.mean(loss_tmp)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def focal_loss1c(pred,tru):\n",
    "    \"\"\"\n",
    "    Focal loss calculated over 1 channel output\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "    gamma = 3\n",
    "    alpha =1\n",
    "    input_soft = F.softmax(to.cat((pred[None,:,:],1-pred[None,:,:]),0),0)+eps #1 channel output\n",
    "    tru = to.cat((tru[None,:,:],1-tru[None,:,:]),0)\n",
    "    \n",
    "    # compute the actual focal loss\n",
    "    weight = to.pow(1. - input_soft, gamma)\n",
    "    focal = -alpha * weight * to.log(input_soft)\n",
    "    loss_tmp = to.sum(tru*focal, dim=1)\n",
    "\n",
    "    loss = to.mean(loss_tmp)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Intersection over Union Loss.\n",
    "    IoU = Area of Overlap / Area of Union\n",
    "    This loss is calculated on our heatmaps.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.EPSILON = 1e-6\n",
    "        \n",
    "    def _op_sum(self, x):\n",
    "        return x.sum(-1).sum(-1)\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        inter = self._op_sum(y_true * y_pred)\n",
    "        union = self._op_sum(y_true ** 2) + self._op_sum(y_pred ** 2) - self._op_sum(y_true * y_pred)\n",
    "        iou = (inter + self.EPSILON) / (union + self.EPSILON)\n",
    "        iou = to.mean(iou)\n",
    "        return 1 - iou\n",
    "        \n",
    "\n",
    "\n",
    "MODEL_NEURONS = 10\n",
    "\n",
    "class ShallowUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A lighter implementation of UNet, it has:\n",
    "    - fewer downsampling blocks\n",
    "    - fewer neurons in the layers\n",
    "    - Batch Normalization is added\n",
    "    \n",
    "    The original UNet paper:\n",
    "    https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.k1 = to.nn.Parameter(to.randn(in_channel,k1sz[0],k1sz[1])) #convolutional filter\n",
    "        self.b1 = to.nn.Parameter(to.randn(in_channel,1,1)) #convolutional offset vector\n",
    "        \n",
    "        \n",
    "        self.down_conv1 = ConvBlock(in_channel, MODEL_NEURONS)\n",
    "        self.down_conv2 = ConvBlock(MODEL_NEURONS, MODEL_NEURONS*2)\n",
    "        self.down_conv3 = ConvBlock(MODEL_NEURONS*2, MODEL_NEURONS*4)\n",
    "        \n",
    "        self.bottle_neck = ConvBlock(MODEL_NEURONS*4, MODEL_NEURONS*8)\n",
    "        \n",
    "        self.up_conv1 = ConvBlock(MODEL_NEURONS*8 + MODEL_NEURONS*4, MODEL_NEURONS*4)\n",
    "        self.up_conv2 = ConvBlock(MODEL_NEURONS*4 + MODEL_NEURONS*2, MODEL_NEURONS*2)\n",
    "        self.up_conv3 = ConvBlock(MODEL_NEURONS + MODEL_NEURONS*2, MODEL_NEURONS)\n",
    "        \n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Conv2d(MODEL_NEURONS, out_channel, kernel_size=3, padding=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Helper classes\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        \n",
    "    def forward(self, im,bins):\n",
    "        \n",
    "        x = F.relu( OGCNN2(im,bins,self.k1)+self.b1 )\n",
    "        \n",
    "        conv_d1 = self.down_conv1(x[None,:,:,:])\n",
    "        conv_d2 = self.down_conv2(self.maxpool(conv_d1))\n",
    "        conv_d3 = self.down_conv3(self.maxpool(conv_d2))\n",
    "        \n",
    "        bottle_neck = self.bottle_neck(self.maxpool(conv_d3))\n",
    "        \n",
    "        conv_u1 = self.up_conv1(to.cat([self.upsample(bottle_neck), conv_d3], dim=1))\n",
    "        conv_u2 = self.up_conv2(to.cat([self.upsample(conv_u1), conv_d2], dim=1))\n",
    "        conv_u3 = self.up_conv3(to.cat([self.upsample(conv_u2), conv_d1], dim=1))\n",
    "        \n",
    "        out = self.conv_out(conv_u3)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "#should we make these.... atrous???\n",
    "# Different filter sizes\n",
    "k1sz = (15,15)\n",
    "k2sz = (10,10)\n",
    "k3sz = (5,5)\n",
    "k4sz = (25,25)\n",
    "\n",
    "#Define the neural network \n",
    "class Net(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Net, self).__init__() \n",
    "        \n",
    "        \n",
    "        # Multi-channel filter. Add more self.ki for more OG layers. \n",
    "        self.k1 = to.nn.Parameter(to.randn(40,k1sz[0],k1sz[1])) #convolutional filter\n",
    "        self.b1 = to.nn.Parameter(to.randn(40,1,1)) #convolutional offset vector\n",
    "        \n",
    "                                 \n",
    "        self.l1 = to.nn.Sequential(\n",
    "            to.nn.Conv2d(40, 80, kernel_size=5, stride=1, padding=1),\n",
    "            to.nn.ReLU(),\n",
    "            to.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            to.nn.Dropout(p=.2)) \n",
    "        self.l2 = to.nn.Sequential(\n",
    "            to.nn.Conv2d(80, 160, kernel_size=3, stride=1, padding=1),\n",
    "            to.nn.ReLU(),\n",
    "            to.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            to.nn.Dropout(p=.15))\n",
    "        self.l3 = to.nn.Sequential(\n",
    "            to.nn.Conv2d(160, 320, kernel_size=3, stride=1, padding=1),\n",
    "            to.nn.ReLU(),\n",
    "            to.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            to.nn.Dropout(p=.05))\n",
    "        self.l4 = to.nn.Sequential(\n",
    "            to.nn.Conv2d(320, 100, kernel_size=3, stride=1, padding=1),\n",
    "            to.nn.ReLU(),\n",
    "            to.nn.Dropout(p=.1))\n",
    "        self.l5 = to.nn.Sequential(\n",
    "            to.nn.Conv2d(100, 1, kernel_size=3, stride=1, padding=1),\n",
    "            to.nn.ReLU(),\n",
    "            to.nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n",
    "            to.nn.Dropout(p=.05))\n",
    "        \n",
    "        self.up = to.nn.Upsample(size = (200,200))\n",
    "    def forward(self,im,bins):\n",
    "        \n",
    "        x = F.relu( OGCNN2(im,bins,self.k1)+self.b1 )\n",
    "        \n",
    "        x = self.l1(x[None,:,:,:]) \n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = self.l5(x)\n",
    "        return self.up(to.sigmoid(x)) #F.hardsigmoid(x) #1-F.relu(1-F.relu(x)) #experimental... forces in 0 to 1 range \n",
    " \n",
    " \n",
    "model = ShallowUNet(20,1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape=False\n",
    "\n",
    "N_KEYPOINTS = 17\n",
    "MODEL_IMG_SIZE = 200\n",
    "\n",
    "\n",
    "trpath = 'HigherHRNet-Human-Pose-Estimation/data/coco/images/train2017_cropped/'\n",
    "trpath_grad = 'openmonkey_annotations/train_grad_cropped/'\n",
    "valpath = 'HigherHRNet-Human-Pose-Estimation/data/coco/images/val2017_cropped/'\n",
    "valpath_grad = 'openmonkey_annotations/val_grad_cropped/'\n",
    "\n",
    "tr_ann,tr_bbox,tr_files = get_annotations('openmonkey_annotations/person_keypoints_train2017.json')\n",
    "val_ann,val_bbox,val_files = get_annotations('openmonkey_annotations/person_keypoints_val2017.json')\n",
    "\n",
    "\n",
    "#train on subset of the data... just for now:\n",
    "per2use = .4\n",
    "extract = np.random.rand(tr_files.shape[0])>(1-per2use)\n",
    "fnames_tr = tr_files[extract]\n",
    "\n",
    "n_tr = fnames_tr.shape[0]\n",
    "\n",
    "tr_ann = tr_ann[extract,:,:]\n",
    "tr_bbox = tr_bbox[extract,:]\n",
    "\n",
    "#fnames_tr = np.array(os.listdir(trpath))\n",
    "\n",
    "extract = np.random.rand(val_files.shape[0])>(1-per2use)\n",
    "fnames_val = val_files[extract]\n",
    "n_val = fnames_val.shape[0]\n",
    "\n",
    "val_ann = val_ann[extract,:,:]\n",
    "val_bbox = val_bbox[extract,:]\n",
    "\n",
    "z = to.zeros(MODEL_IMG_SIZE,MODEL_IMG_SIZE)\n",
    "    \n",
    "val_ann2 = 0*val_ann\n",
    "val_ann_ims = []\n",
    "for i in range(val_ann.shape[0]):\n",
    "    ai = MODEL_IMG_SIZE*(val_ann[i]-val_bbox[i,0:2])/val_bbox[i,2:]\n",
    "    ai[ai>=200] = 199\n",
    "    val_ann2[i,:,:] = ai\n",
    "\n",
    "tr_tot = np.shape(tr_files)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims_val = []\n",
    "bins_val = []\n",
    "\n",
    "for i in range(n_val):\n",
    "    ims_val.append(cv.imread(valpath + fnames_val[i][0:-4]+'.jpg',0))\n",
    "    bins_val.append(cv.imread(valpath_grad + fnames_val[i][0:-4]+'.png',0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, config, scheduler=None):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = {\"train\": [], \"val\": []}\n",
    "        self.epochs = config['epochs']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.batches_per_epoch = config['batches_per_epoch']\n",
    "        self.batches_per_epoch_val = config['batches_per_epoch_val']\n",
    "        self.device = config['device']\n",
    "        self.scheduler = scheduler\n",
    "        self.checkpoint_frequency = 100\n",
    "        self.early_stopping_epochs = 10\n",
    "        self.early_stopping_avg = 10\n",
    "        self.early_stopping_precision = 8\n",
    "        \n",
    "        self.to_tensor = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self._epoch_train()\n",
    "            self._epoch_eval()\n",
    "            print(\n",
    "                \"Epoch: {}/{}, Train Loss={}, Val Loss={}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.epochs,\n",
    "                    np.round(self.loss[\"train\"][-1], 10),\n",
    "                    np.round(self.loss[\"val\"][-1], 10)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(self.loss['train'][-1])\n",
    "                \n",
    "            # Saving the model\n",
    "            if (epoch+1) % self.checkpoint_frequency == 0:\n",
    "                to.save(self.model.state_dict(), \"model_{}\".format(str(epoch+1).zfill(3)))\n",
    "                \n",
    "            # early stopping\n",
    "            if epoch < self.early_stopping_avg:\n",
    "                min_val_loss = np.round(np.mean(self.loss['val']), self.early_stopping_precision)\n",
    "                no_decrease_epochs = 0\n",
    "                \n",
    "            else:\n",
    "                val_loss = np.round(np.mean(self.loss['val'][-self.early_stopping_avg:]), self.early_stopping_precision)\n",
    "                \n",
    "                if val_loss >= min_val_loss:\n",
    "                    no_decrease_epochs += 1\n",
    "                else:\n",
    "                    min_val_loss = val_loss\n",
    "                    no_decrease_epochs = 0\n",
    "                    \n",
    "            \n",
    "            if no_decrease_epochs > self.early_stopping_epochs:\n",
    "                print(\"Early Stopping\")\n",
    "                break\n",
    "                \n",
    "        to.save(self.model.state_dict(), \"model_final\")\n",
    "        return self.model\n",
    "        \n",
    "        \n",
    "    def _epoch_train(self):\n",
    "        self.model.train()\n",
    "        running_loss = []\n",
    "        \n",
    "        for i in range(self.batches_per_epoch):\n",
    "            \n",
    "            loss = 0\n",
    "            self.optimizer.zero_grad()\n",
    "            idxes = np.random.randint(low=0, high=n_tr, size=(self.batch_size,))\n",
    "            for j in idxes:\n",
    "                heatmaps = vector_to_heatmaps(tr_ann2[i,:,:])\n",
    "                labels = to.tensor(heatmaps).to(cuda0)\n",
    "                \n",
    "                outputs = model(ims_tr[j], bins_tr[j])\n",
    "                loss += criterion(outputs, labels)\n",
    "                \n",
    "            loss /= self.batch_size\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "        \n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss['train'].append(epoch_loss)\n",
    "                \n",
    "    def _epoch_eval(self):\n",
    "        self.model.eval()\n",
    "        running_loss = []\n",
    "        \n",
    "        with to.no_grad():\n",
    "            for i in range(self.batches_per_epoch_val):\n",
    "                loss = 0\n",
    "                idxes = np.random.randint(low=0, high=n_val, size=(self.batch_size,))\n",
    "                for j in idxes:\n",
    "                    heatmaps = vector_to_heatmaps(val_ann2[i,:,:])\n",
    "                    labels = to.tensor(heatmaps).to(cuda0)\n",
    "                \n",
    "                    output = model(ims_val[j], bins_val[j])\n",
    "                    loss += criterion(output, labels)\n",
    "                    \n",
    "                loss /= self.batch_size\n",
    "                running_loss.append(loss.item())\n",
    "                \n",
    "            epoch_loss = np.mean(running_loss)\n",
    "            self.loss['val'].append(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": 48,\n",
    "    \"batches_per_epoch\": 50,\n",
    "    \"batches_per_epoch_val\": 20,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "model = Net()\n",
    "model.to(cuda0)\n",
    "criterion = FocalLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)\n",
    "# optimizer = optim.Adadelta(model.parameters(), config[\"learning_rate\"])\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, factor=0.5, patience=20, verbose=True, threshold=0.00001\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, criterion, optimizer, config, scheduler)\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Code for metrics evaluation\n",
    "\n",
    "\n",
    "<br>Calculating the ki values:\n",
    "<br>This is inspired from: https://cocodataset.org/#keypoints-eval\n",
    "<br>They use a scale factor, which we've taken as the width of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros(17)\n",
    "ki = np.zeros(17)\n",
    "N = len(train_keypoints['data'])\n",
    "\n",
    "for annot in train_keypoints['data']:\n",
    "    keypoints = annot['landmarks']\n",
    "    width = annot['bbox'][2]\n",
    "    \n",
    "    for i in range(0,17*2,2):\n",
    "        sigma_squared = (keypoints[i]**2 + keypoints[i+1]**2) / width**2\n",
    "        ki[i//2] += sigma_squared\n",
    "\n",
    "ki = ki / (N)\n",
    "ki = np.sqrt(ki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these ki values to calculate our metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(pred_path, actual_path):\n",
    "\n",
    "    with open(pred_path) as f:\n",
    "        val_keypoint_results = json.load(f)\n",
    "\n",
    "    with open(actual_path) as g:\n",
    "        val_keypoints_actual = json.load(g)\n",
    "\n",
    "    actual_key_map = {}\n",
    "    for annot in val_keypoints_actual['annotations']:\n",
    "        actual_key_map[annot['image_id']] = (annot['keypoints'], annot['bbox'][2])\n",
    "\n",
    "    coco_ks = ki\n",
    "    pos_to_idx = {\n",
    "        0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10, 11:11, 12:12, 13:13, 14:14, 15:15, 16:16\n",
    "    }\n",
    "\n",
    "    keypoint_kis = np.zeros(17)\n",
    "    for i in range(17):\n",
    "        keypoint_kis[i] = coco_ks[pos_to_idx[i]]\n",
    "\n",
    "    J = len(val_keypoint_results)\n",
    "\n",
    "    pck_epsilon = 0.02\n",
    "    pa_epsilon = 0.02\n",
    "\n",
    "    MPJPE = np.zeros(17)\n",
    "    PCK = 0\n",
    "    PA = 0\n",
    "\n",
    "    for i in range(J):\n",
    "        img_id = val_keypoint_results[0]['image_id']\n",
    "        width = actual_key_map[img_id][1]\n",
    "        actual_keypoints = actual_key_map[img_id][0]\n",
    "        pred_keypoints = val_keypoint_results[0]['keypoints']\n",
    "\n",
    "        for j in range(0,17*3,3):\n",
    "            dist = np.sqrt((actual_keypoints[j]-pred_keypoints[j])**2 + (actual_keypoints[j+1]-pred_keypoints[j+1])**2) / width\n",
    "            MPJPE[j//3] += dist\n",
    "            PCK += 1 if dist < pck_epsilon else 0\n",
    "\n",
    "            num = -((actual_keypoints[j]-pred_keypoints[j])**2 + (actual_keypoints[j+1]-pred_keypoints[j+1])**2)\n",
    "            denm = 2 * (width**2) * (keypoint_kis[j//3]**2)\n",
    "            normal = np.exp(num/denm)\n",
    "            PA += 1 if normal >= pa_epsilon else 0\n",
    "\n",
    "    MPJPE = MPJPE / J\n",
    "    PCK = PCK / (17*J)\n",
    "    PA = PA / (17*J)\n",
    "\n",
    "    return MPJPE, PCK, PA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Generate FCN-ResNet masks for the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "fcn_model = to.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)\n",
    "fcn_model.eval()\n",
    "\n",
    "input_image = Image.open(trpath + fnames_tr[100])\n",
    "input_image = input_image.convert(\"RGB\")\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "input_batch = input_batch.to('cuda')\n",
    "fcn_model.to('cuda')\n",
    "\n",
    "with to.no_grad():\n",
    "    output = fcn_model(input_batch)\n",
    "output_predictions = output['out'][0].argmax(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the masks to the images and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PIL import Image\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "fcn_model = to.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)\n",
    "fcn_model.eval()\n",
    "\n",
    "destpath = 'openmonkey_annotations/masked_val_full/'\n",
    "baseline_path = 'HigherHRNet-Human-Pose-Estimation/data/coco/images/val2017_cropped/'\n",
    "grad_path = 'openmonkey_annotations/val_grad_cropped/'\n",
    "dest_grad_path = 'openmonkey_annotations/masked_grad_val/'\n",
    "\n",
    "file_names = os.listdir(baseline_path)\n",
    "\n",
    "for file_name in file_names:\n",
    "\n",
    "    input_image = Image.open(baseline_path + file_name)\n",
    "    input_image = input_image.convert(\"RGB\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    fcn_model.to('cuda')\n",
    "\n",
    "    with to.no_grad():\n",
    "        output = fcn_model(input_batch)\n",
    "    output_predictions = output['out'][0].argmax(0)\n",
    "    r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
    "\n",
    "    kernel = np.ones((30, 30), 'uint8')\n",
    "    dilated_img = cv.dilate( np.array(r), kernel)\n",
    "\n",
    "    masked_img = np.array(input_image)\n",
    "    masked_img[:,:,0][dilated_img <= 0] = 0\n",
    "    masked_img[:,:,1][dilated_img <= 0] = 0\n",
    "    masked_img[:,:,2][dilated_img <= 0] = 0\n",
    "    \n",
    "    img_id = re.findall(r'[0-9]+', file_name)[0]\n",
    "    new_file_name = 'val_%07d.png' % int(img_id)\n",
    "    grad_bin = cv.imread(grad_path + new_file_name)\n",
    "    \n",
    "    grad_bin[dilated_img <= 0] = 0\n",
    "\n",
    "    plt.imsave(destpath + file_name, masked_img)\n",
    "    plt.imsave(dest_grad_path + new_file_name, grad_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bins were not saved correctly, because PIL's Image.open reads it incorrectly. Using cv.imread gives correct results. The above code has been modified now, but you can use the one below to make the correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'openmonkey_annotations/masked_train_full/'\n",
    "grad_path = 'openmonkey_annotations/train_grad_cropped/'\n",
    "dest_grad_path = 'masked_grad_train/'\n",
    "\n",
    "file_names = os.listdir(img_path)\n",
    "\n",
    "for file_name in file_names:\n",
    "    masked_img = cv.imread(img_path + file_name)\n",
    "\n",
    "    img_id = re.findall(r'[0-9]+', file_name)[0]\n",
    "    new_file_name = 'train_%07d.png' % int(img_id)\n",
    "    grad_bin = cv.imread(grad_path + new_file_name)\n",
    "    grad_bin[masked_img <= 0] = 0\n",
    "    cv.imwrite(new_file_name, grad_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Helper code to convert the Simple Baseline output file to OMC output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_path = \"openmonkey_annotations/test_prediction.json\"\n",
    "res_path = \"coco_annotations/results/SimpleBaseline/Trained_resnet_50_test_ogcnn/keypoints_val2017_results.json\"\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "    \n",
    "res_map = {}\n",
    "for ann in annotations:\n",
    "    res_map[ann[\"image_id\"]] = ann[\"keypoints\"]\n",
    "\n",
    "    \n",
    "with open(act_path, 'r') as g:\n",
    "    actual = json.load(g)\n",
    "\n",
    "data = []\n",
    "\n",
    "for ann in actual[\"data\"]:\n",
    "    temp = ann\n",
    "    img_id = int(ann[\"file\"].split(\"_\")[1].split(\".\")[0])\n",
    "    pred_kps = res_map[img_id]\n",
    "    \n",
    "    kps = []\n",
    "    for i in range(0,len(pred_kps),3):\n",
    "        kps.extend([pred_kps[i] + ann[\"bbox\"][0], pred_kps[i+1] + ann[\"bbox\"][1]])\n",
    "\n",
    "    temp[\"landmarks\"] = kps\n",
    "    data.append(temp)\n",
    "    \n",
    "result = {\n",
    "    \"data\": data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"coco_annotations/results/SimpleBaseline/Trained_resnet_50_test_ogcnn/test_prediction.json\", 'w') as fp:\n",
    "    json.dump(result, fp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "monkey_ogcnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
